import os
import json
import sys
import time
import torch
from typing import List, Dict, Any
from pathlib import Path
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain_classic.chains import RetrievalQA
from langchain_core.documents import Document
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline,
)

os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "7200"  
os.environ["CURL_CA_BUNDLE"] = ""              # æ¸¬è©¦æ™‚ä½¿ç”¨
os.environ["TOKENIZERS_PARALLELISM"] = "false" # å–®åŸ·è¡Œç·’
class Config:
    DOCUMENTS_DIR = "documents"                # æ³•è¦çŸ¥è­˜åº«
    CONTRACTS_DIR = "contracts"                # å¥‘ç´„æ–‡ä»¶
    LAW_JSON_PATH = "documents/new_law.json"   # æ³•è¦æ–‡ä»¶
    CHUNK_SIZE = 800                           # æŠŠæ³•æ¢åˆ‡å¡Š
    CHUNK_OVERLAP = 100                        # é¿å…èªæ„ä¸é€£çºŒ
    EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    LLM_MODEL_NAME = "yentinglin/Taiwan-LLM-7B-v2.1-chat"
    VECTOR_DB_DIR = "lawvector_db"             # å‘é‡åº«ä½ç½®
    TOP_K = 5                                  # æ‰¾å‡ºå¹¾æ¢ç›¸é—œçš„
    USE_4BIT_QUANTIZATION = False              # è¦ä¸è¦é‡åŒ–

class LaborContractReviewSystem:               # å¤–ç±å‹å·¥å¥‘ç´„å¯©æŸ¥
    def __init__(self, config: Config):
        self.config = config
        self.embeddings = None
        self.llm = None
        self.vector_db = None
        self.qa_chain = None
        os.makedirs(config.DOCUMENTS_DIR, exist_ok=True)
        os.makedirs(config.CONTRACTS_DIR, exist_ok=True)
        self._init_embeddings()
        self._init_llm()
    
    def _init_embeddings(self):
        print(f"è¼‰å…¥å‘é‡æ¨¡å‹")
        max_retries = 3
        for attempt in range(max_retries):
            try:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=self.config.EMBEDDING_MODEL_NAME,
                    model_kwargs={
                        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
                        'trust_remote_code': True  
                    },
                    encode_kwargs={
                        'normalize_embeddings': True,  # åšæ­£è¦åŠƒ
                        'batch_size': 16 
                    }
                )
                break
            except Exception as e:
                print(f"Failed: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    raise
        print("Successful\n")
    
    def _init_llm(self):
        print(f"è¼‰å…¥LLM")
        max_retries = 3
        for attempt in range(max_retries):
            try:
                tokenizer = AutoTokenizer.from_pretrained(
                    self.config.LLM_MODEL_NAME,
                    trust_remote_code=True,
                    resume_download=True
                )
                if tokenizer.pad_token is None:  #æ–¹ä¾¿æ‰¹æ¬¡è™•ç†
                    tokenizer.pad_token = tokenizer.eos_token
                model = AutoModelForCausalLM.from_pretrained(
                    self.config.LLM_MODEL_NAME,
                    device_map="auto",
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    trust_remote_code=True,  # å…è¨±æ¨¡å‹ç”¨ä»–å€‘è‡ªå·±çš„Python code
                    resume_download=True     # ä¸‹è¼‰ä¸­æ–·ï¼Œæœƒå¾æ–·é»çºŒå‚³ 
                )
                pipe = pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    max_new_tokens=1200,  
                    temperature=0.5,         # é™ä½é æ¸¬æ–‡å­—çš„éš¨æ©Ÿæ€§
                    repetition_penalty=1.3,  # é™ä½é‡è¤‡å­—è©
                    do_sample=True,          # è¦ä¸è¦é€²è¡Œå¤šé …å¼æ¡æ¨£
                    top_p=0.9,              # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬å¤šæ¨£æ€§
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
                self.llm = HuggingFacePipeline(pipeline=pipe)
                break  
            except Exception as e:
                print(f"å¤±æ•—: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    raise
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            print(f"GPU é¡¯å­˜ä½¿ç”¨: {allocated:.2f} GB\n")
        print("Successful\n")
    
    def load_law_json(self) -> List[Document]:
        print(f"è¼‰å…¥JSON")
        documents = []
        json_files = list(Path(self.config.DOCUMENTS_DIR).glob('*.json'))
        if not json_files:
            print(f"åœ¨ {self.config.DOCUMENTS_DIR} ä¸­æ‰¾ä¸åˆ°ä»»ä½• JSON æª”æ¡ˆ")
            return []
        print(f"æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")

        for json_path in json_files:
            try:
                print(f"æ­£åœ¨è®€å–: {json_path.name}")
                with open(json_path, 'r', encoding='utf-8') as f:
                    law_data = json.load(f)
                if isinstance(law_data, list):
                    for item in law_data:
                        content = self._format_law_item(item)
                        doc = Document(
                            page_content=content,
                            metadata={
                                "source": json_path.name, 
                                "type": "labor_law",
                                **item  
                            }
                        )
                        documents.append(doc)
                elif isinstance(law_data, dict):
                    for law_name, law_content in law_data.items():
                        if isinstance(law_content, list):
                            for item in law_content:
                                content = self._format_law_item(item)
                                doc = Document(
                                    page_content=content,
                                    metadata={
                                        "source": json_path.name, 
                                        "law_name": law_name,
                                        "type": "labor_law"
                                    }
                                )
                                documents.append(doc)
            except Exception as e:
                print(f"è®€å– {json_path.name} å¤±æ•—: {e}")
                continue 
        return documents
    
    def _format_law_item(self, item: Dict) -> str:  # æ ¼å¼åŒ–æ³•è¦æ¢æ–‡
        parts = []
        if 'instruction' in item and 'input' in item:
            parts.append(f"ã€å•é¡Œã€‘{item['instruction']}")
            parts.append(f"ã€æ³•è¦ä¾æ“šã€‘{item['input']}")
            if 'output' in item:
                parts.append(f"ã€èªªæ˜ã€‘{item['output']}")
            return '\n'.join(parts)
        if 'æ³•è¦åç¨±' in item:
            parts.append(f"ã€{item['æ³•è¦åç¨±']}ã€‘")
        if 'æ¢è™Ÿ' in item:
            parts. append(f"ç¬¬ {item['æ¢è™Ÿ']} æ¢")
        if 'æ¢æ–‡å…§å®¹' in item:
            parts.append(item['æ¢æ–‡å…§å®¹'])
        if 'èªªæ˜' in item:
            parts.append(f"èªªæ˜ï¼š{item['èªªæ˜']}")
        if not parts:
            text = str(item)
            if len(text) > 500:
                text = text[:500] + "..."
            parts.append(text)
        return '\n'.join(parts)
    
    def build_law_knowledge_base(self):
        print("å»ºç«‹æ³•è¦çŸ¥è­˜åº«")
        law_documents = self.load_law_json()
        if os.path.exists(self.config.DOCUMENTS_DIR):
            print(f"æŸ¥è©¢å…¶ä»–æ³•è¦æ–‡ä»¶")
            loader = DirectoryLoader(
                self.config.DOCUMENTS_DIR,
                glob="**/*.txt",
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"}
            )
            text_documents = loader.load()
            print(f"ç¸½å…± {len(text_documents)} ä»½å…¶å®ƒæ–‡æœ¬æ–‡ä»¶")
            law_documents.extend(text_documents)
        if not law_documents:
            raise ValueError("æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ³•è¦")
        print(f"\nç¸½å…± {len(law_documents)} ä»½æ³•è¦æ–‡ä»¶")
        text_splitter = RecursiveCharacterTextSplitter(   # åˆ†å‰²æ–‡æœ¬
            chunk_size=self.config.CHUNK_SIZE,
            chunk_overlap=self.config.CHUNK_OVERLAP,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼Œ", " ", ""],
            length_function=len,
            is_separator_regex=False
        )
        texts = text_splitter.split_documents(law_documents)
        self.vector_db = Chroma.from_documents(          # 4.å‰µå»ºå‘é‡åº«
            documents=texts,
            embedding=self.embeddings,
            persist_directory=self.config.VECTOR_DB_DIR,
            collection_name="law_collection"
        )
        self._create_qa_chain()
        print(f"\nçŸ¥è­˜åº«å»ºç«‹å®Œæˆ")
    
    def _create_qa_chain(self):                         # å•ç­”æª¢ç´¢éˆ
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_db.as_retriever(
                search_type="similarity",
                search_kwargs={"k": self.config.TOP_K}
            ),
            return_source_documents=True,
            verbose=False
        )
    
    def load_existing_knowledge_base(self) -> bool:
        if os.path.exists(self.config.VECTOR_DB_DIR):
            print(f"\nå·²ç¶“æœ‰ç¾æœ‰çŸ¥è­˜åº«")
            try:
                self.vector_db = Chroma(
                    persist_directory=self.config.VECTOR_DB_DIR,
                    embedding_function=self.embeddings,
                    collection_name="law_collection"
                )
                self._create_qa_chain()
                print("Successful\n")
                return True
            except Exception as e:
                print(f"Failed: {e}\n")
                return False
        return False
    
    def review_contract(self, contract_path: str) -> Dict[str, Any]:
        print(f"å¯©æŸ¥å¥‘ç´„")
        try:  # è®€å–å¥‘ç´„å…§å®¹
            with open(contract_path, 'r', encoding='utf-8') as f:
                contract_content = f.read()
        except Exception as e:
            return {"Failed ": f"{e}"}
        
        # ğŸ¯ é™åˆ¶å¥‘ç´„é•·åº¦ï¼Œé¿å…è¶…é token é™åˆ¶
        max_chars = 2000
        if len(contract_content) > max_chars:
            print(f"âš ï¸ å¥‘ç´„éé•· ({len(contract_content)} å­—å…ƒ)ï¼Œæˆªæ–·è‡³ {max_chars} å­—å…ƒ")
            contract_content = contract_content[:max_chars]



        review_questions = [                                        # æ§‹é€ å¯©æŸ¥å•é¡Œ(å¯æ–°å¢æ›´å¤šè§’åº¦æœ€å¤šäº”å€‹)
            f"""
                ä½ æ˜¯å°ç£å‹å‹•æ³•å°ˆå®¶ï¼Œè«‹å¯©æŸ¥ä»¥ä¸‹å¤–ç±å‹å·¥å¥‘ç´„ã€‚

                ã€å¥‘ç´„å…§å®¹ã€‘
                {contract_content}

                ã€å¯©æŸ¥ä»»å‹™ã€‘
                è«‹é€é …æª¢æŸ¥ä¸¦ä»¥æ­¤æ ¼å¼å›ç­”ï¼š

                åˆæ³•é …ç›®
                - [åˆ—å‡ºç¬¦åˆæ³•è¦çš„æ¢æ¬¾]

                é•æ³•é …ç›®
                - [åˆ—å‡ºé•æ³•æ¢æ¬¾ï¼Œæ ¼å¼ï¼šé•æ³•å…§å®¹ | é•åæ³•æ¢ | ç½°å‰‡]

                å»ºè­°ä¿®æ”¹
                - [åˆ—å‡ºéœ€æ”¹é€²çš„æ¢æ¬¾]

                é‡é»æª¢æŸ¥ï¼š
                1. å·¥è³‡æ˜¯å¦ â‰¥ 27,470å…ƒ
                2. å·¥æ™‚æ˜¯å¦ â‰¤ 8å°æ™‚/æ—¥ã€40å°æ™‚/é€±
                3. ä¼‘å‡æ˜¯å¦ç¬¦åˆæ¯é€±è‡³å°‘1æ—¥
                4. é•ç´„é‡‘æ˜¯å¦åˆç†
                5. æ˜¯å¦æŠ•ä¿å‹å¥ä¿

                åªæ ¹æ“šå¥‘ç´„å¯¦éš›å…§å®¹å›ç­”ï¼Œä¸è¦æ¨æ¸¬ã€‚,
            """
            "è«‹åˆ—å‡ºæ­¤å¥‘ç´„ä¸­å°å‹å·¥ä¸åˆ©çš„æ¢æ¬¾ã€‚",
            "è«‹è©•ä¼°æ­¤å¥‘ç´„æ˜¯å¦ç¬¦åˆå‹åŸºæ³•ã€‚"
        ]
        results = {
            "contract_path": contract_path,
            "contract_length": len(contract_content),
            "reviews": [],
            "related_laws": []
        }
        
        for idx, question in enumerate(review_questions, 1):        # åŸ·è¡Œå¤šè§’åº¦å¯©æŸ¥
            print(f"åŸ·è¡Œå¯©æŸ¥")
            try:
                start_time = time.time()
                result = self.qa_chain.invoke({"query": question})  # RAGï¼šæ‰¾æ³•æ¢  LLMï¼šå¯«æ³•å¾‹åˆ†æ
                end_time = time.time()
                answer = result.get("result", "")                   # LLM å›ç­”
                answer = self._clean_answer(answer)
                
                review_item = {
                    "question_type": f"å¯©æŸ¥è§’åº¦ {idx}",
                    "answer": answer,
                    "processing_time": end_time - start_time,
                    "source_laws": []
                }
                
                unique_law_set = set() 
                for doc in result.get("source_documents", []):
                    law_info = {
                        "content": doc.page_content[:300],
                        "source": doc.metadata.get("source", "Unknown"),
                        "metadata": doc.metadata
                    }
                    review_item["source_laws"].append(law_info)  # æ¯ä¸€å€‹å¯©æŸ¥è§’åº¦è‡ªå·±åƒè€ƒçš„æ³•è¦æ–‡ä»¶
                    unique_key = f"{law_info['source']}:{id(doc)}"
                    if unique_key not in unique_law_set:         # æ‰€æœ‰å¯©æŸ¥è§’åº¦ç”¨åˆ°çš„æ³•è¦ï¼Œä½†ä¸é‡è¤‡
                        unique_law_set.add(unique_key)
                        results["related_laws"].append(law_info)
                results["reviews"].append(review_item)
                print(f"Successful\n")
            except Exception as e:
                print(f"Failed: {e}\n")
                results["reviews"].append({
                    "question_type": f"å¯©æŸ¥è§’åº¦ {idx}",
                    "error": str(e)
                })
        return results
    
    def _clean_answer(self, answer: str) -> str:
        if "Use the following pieces of context" in answer:
            parts = answer.split("Helpful Answer:")
            if len(parts) > 1:
                answer = parts[-1]
        if "Question:" in answer:
            parts = answer.split("Question:")
            if len(parts) > 1:
                answer = parts[0]
        for marker in ["<|im_start|>", "<|im_end|>", "<s>", "</s>"]:
            answer = answer.replace(marker, "")
        return answer.strip()
    
    def batch_review_contracts(self, contracts_dir: str = None):  # æœ€å¾Œæœƒç”¢å‡ºä¸€å€‹ report.txt
        if contracts_dir is None:
            contracts_dir = self.config.CONTRACTS_DIR
        print(f"æƒæå¥‘ç´„ç›®éŒ„")
        contract_files = []

        for ext in ['*.txt', '*.doc', '*.docx']:
            contract_files.extend(Path(contracts_dir).glob(ext))
        if not contract_files:
            print(f"{contracts_dir} ä¸­æ²’æœ‰æ‰¾åˆ°å¥‘ç´„æ–‡ä»¶")
            return []
        print(f"æ‰¾åˆ° {len(contract_files)} ä»½å¥‘ç´„\n")

        all_results = []
        for idx, contract_file in enumerate(contract_files, 1):
            print(f"å¯©æŸ¥é€²åº¦: {idx}/{len(contract_files)}")            
            result = self.review_contract(str(contract_file))
            all_results.append(result)

        return all_results
    
    def generate_review_report(self, results: List[Dict], output_path: str = "report.txt"):
        print(f"ç”Ÿæˆå¯©æŸ¥å ±å‘Š")
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("å¤–ç±å‹å·¥å¥‘ç´„å¯©æŸ¥å ±å‘Š\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"å¯©æŸ¥å¥‘ç´„æ•¸: {len(results)} ä»½\n")
            f.write("=" * 80 + "\n\n")
            for idx, result in enumerate(results, 1):
                f.write(f"\n{'#' * 80}\n")
                f.write(f"å¥‘ç´„ {idx}: {result.get('contract_path', 'Unknown')}\n")
                f.write(f"{'#' * 80}\n\n")
                if 'error' in result:
                    f.write(f"Failed: {result['error']}\n")
                    continue

                for review_idx, review in enumerate(result.get('reviews', []), 1):   # å¯«å…¥å¯©æŸ¥çµæœ
                    f.write(f"\n{'-' * 80}\n")
                    f.write(f"ã€{review['question_type']}ã€‘\n")
                    f.write(f"{'-' * 80}\n\n")
                    if 'error' in review:
                        f.write(f"Failed: {review['error']}\n")
                    else:
                        f.write(f"{review['answer']}\n\n")
                        f.write(f"è™•ç†æ™‚é–“: {review['processing_time']:.2f} ç§’\n")
                        f.write(f"åƒè€ƒæ³•è¦: {len(review. get('source_laws', []))} æ¢\n")
                
                f.write(f"\n{'=' * 80}\n")
                f.write("åƒè€ƒæ³•è¦æ¢æ–‡\n")
                f.write(f"{'=' * 80}\n\n")
                for law_idx, law in enumerate(result.get('related_laws', []), 1):
                    content = law['content']
                    if len(content) > 300:
                        content = content[:300] + "..."
                    f.write(f"{law_idx}. {content}\n")
                    f.write(f"   ä¾†æº: {law['source']}\n\n")
        
        print(f"å ±å‘Šå·²ä¿å­˜è‡³: {output_path}\n")


def main():
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    print("å¤–ç±å‹å·¥å¥‘ç´„å¯©æŸ¥ç³»çµ±")
    config = Config()
    try:
        system = LaborContractReviewSystem(config)
        if not system.load_existing_knowledge_base():  # å»ºç«‹æ³•è¦çŸ¥è­˜åº«
            system.build_law_knowledge_base()
        print("é–‹å§‹å¯©æŸ¥å¥‘ç´„")

        if len(sys.argv) > 1:  # å–®ä¸€æª”æ¡ˆæ¨¡å¼
            contract_file = sys.argv[1]
            print(f"è™•ç†å–®ä¸€å¥‘ç´„: {contract_file}")
            if not os.path.exists(contract_file):
                print(f"Failed: {contract_file}")
                sys.exit(1)
            result = system.review_contract(contract_file)
            system.generate_review_report([result], "report.txt")
            print("Successfulï¼")
            
        else:  # æ‰¹æ¬¡è™•ç†æ¨¡å¼
            results = system.batch_review_contracts()
            if results:
                system.generate_review_report(results, "report.txt")
                print("Successfulï¼")
                print(f"å…±å¯©æŸ¥ {len(results)} ä»½å¥‘ç´„")
            else:
                print("Failed")
                sys.exit(1)
    except Exception as e:
        print(f"\nFailed: {e}")
        import traceback
        traceback.print_exc()
if __name__ == "__main__":
    main()