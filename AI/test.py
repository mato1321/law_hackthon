import os
import json
import sys
import time
import torch
from typing import List, Dict, Any
from pathlib import Path
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain_classic.chains import RetrievalQA
from langchain_core.documents import Document
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline,
)

os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "7200"  
os.environ["CURL_CA_BUNDLE"] = ""              # æ¸¬è©¦æ™‚ä½¿ç”¨
os.environ["TOKENIZERS_PARALLELISM"] = "false" # å–®åŸ·è¡Œç·’
class Config:
    DOCUMENTS_DIR = "documents"                # æ³•è¦çŸ¥è­˜åº«
    CONTRACTS_DIR = "contracts"                # å¥‘ç´„æ–‡ä»¶
    LAW_JSON_PATH = "documents/new_law.json"   # æ³•è¦æ–‡ä»¶
    CHUNK_SIZE = 800                           # æŠŠæ³•æ¢åˆ‡å¡Š
    CHUNK_OVERLAP = 100                        # é¿å…èªæ„ä¸é€£çºŒ
    EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
    LLM_MODEL_NAME = "yentinglin/Taiwan-LLM-7B-v2.1-chat"
    VECTOR_DB_DIR = "lawvector_db"             # å‘é‡åº«ä½ç½®
    TOP_K = 5                                  # æ‰¾å‡ºå¹¾æ¢ç›¸é—œçš„
    USE_4BIT_QUANTIZATION = False              # è¦ä¸è¦é‡åŒ–

class LaborContractReviewSystem:               # å¤–ç±å‹å·¥å¥‘ç´„å¯©æŸ¥
    def __init__(self, config: Config):
        self.config = config
        self.embeddings = None
        self.llm = None
        self.vector_db = None
        self.qa_chain = None
        os.makedirs(config.DOCUMENTS_DIR, exist_ok=True)
        os.makedirs(config.CONTRACTS_DIR, exist_ok=True)
        self._init_embeddings()
        self._init_llm()
    
    def _init_embeddings(self):
        print(f"è¼‰å…¥å‘é‡æ¨¡å‹")
        max_retries = 3
        for attempt in range(max_retries):
            try:
                self.embeddings = HuggingFaceEmbeddings(
                    model_name=self.config.EMBEDDING_MODEL_NAME,
                    model_kwargs={
                        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
                        'trust_remote_code': True  
                    },
                    encode_kwargs={
                        'normalize_embeddings': True,  # åšæ­£è¦åŠƒ
                        'batch_size': 16 
                    }
                )
                break
            except Exception as e:
                print(f"Failed: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    raise
        print("Successful\n")
    
    def _init_llm(self):
        print(f"è¼‰å…¥LLM")
        max_retries = 3
        for attempt in range(max_retries):
            try:
                tokenizer = AutoTokenizer.from_pretrained(
                    self.config.LLM_MODEL_NAME,
                    trust_remote_code=True,
                    resume_download=True
                )
                if tokenizer.pad_token is None:  #æ–¹ä¾¿æ‰¹æ¬¡è™•ç†
                    tokenizer.pad_token = tokenizer.eos_token
                model = AutoModelForCausalLM.from_pretrained(
                    self.config.LLM_MODEL_NAME,
                    device_map="auto",
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    trust_remote_code=True,  # å…è¨±æ¨¡å‹ç”¨ä»–å€‘è‡ªå·±çš„Python code
                    resume_download=True     # ä¸‹è¼‰ä¸­æ–·ï¼Œæœƒå¾æ–·é»çºŒå‚³ 
                )
                pipe = pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    max_new_tokens=1200,  
                    temperature=0.5,         # é™ä½é æ¸¬æ–‡å­—çš„éš¨æ©Ÿæ€§
                    repetition_penalty=1.3,  # é™ä½é‡è¤‡å­—è©
                    do_sample=True,          # è¦ä¸è¦é€²è¡Œå¤šé …å¼æ¡æ¨£
                    top_p=0.9,              # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬å¤šæ¨£æ€§
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
                self.llm = HuggingFacePipeline(pipeline=pipe)
                break  
            except Exception as e:
                print(f"å¤±æ•—: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    raise
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            print(f"GPU é¡¯å­˜ä½¿ç”¨: {allocated:.2f} GB\n")
        print("Successful\n")
    
    def load_law_json(self) -> List[Document]:
        print(f"è¼‰å…¥JSON")
        documents = []
        json_files = list(Path(self.config.DOCUMENTS_DIR).glob('*.json'))
        if not json_files:
            print(f"åœ¨ {self.config.DOCUMENTS_DIR} ä¸­æ‰¾ä¸åˆ°ä»»ä½• JSON æª”æ¡ˆ")
            return []
        print(f"æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")

        for json_path in json_files:
            try:
                print(f"æ­£åœ¨è®€å–: {json_path.name}")
                with open(json_path, 'r', encoding='utf-8') as f:
                    law_data = json.load(f)
                if isinstance(law_data, list):
                    for item in law_data:
                        content = self._format_law_item(item)
                        doc = Document(
                            page_content=content,
                            metadata={
                                "source": json_path.name, 
                                "type": "labor_law",
                                **item  
                            }
                        )
                        documents.append(doc)
                elif isinstance(law_data, dict):
                    for law_name, law_content in law_data.items():
                        if isinstance(law_content, list):
                            for item in law_content:
                                content = self._format_law_item(item)
                                doc = Document(
                                    page_content=content,
                                    metadata={
                                        "source": json_path.name, 
                                        "law_name": law_name,
                                        "type": "labor_law"
                                    }
                                )
                                documents.append(doc)
            except Exception as e:
                print(f"è®€å– {json_path.name} å¤±æ•—: {e}")
                continue 
        return documents
    
    def _format_law_item(self, item: Dict) -> str:  # æ ¼å¼åŒ–æ³•è¦æ¢æ–‡
        parts = []
        if 'instruction' in item and 'input' in item:
            parts.append(f"ã€å•é¡Œã€‘{item['instruction']}")
            parts.append(f"ã€æ³•è¦ä¾æ“šã€‘{item['input']}")
            if 'output' in item:
                parts.append(f"ã€èªªæ˜ã€‘{item['output']}")
            return '\n'.join(parts)
        if 'æ³•è¦åç¨±' in item:
            parts.append(f"ã€{item['æ³•è¦åç¨±']}ã€‘")
        if 'æ¢è™Ÿ' in item:
            parts.append(f"ç¬¬ {item['æ¢è™Ÿ']} æ¢")
        if 'æ¢æ–‡å…§å®¹' in item:
            parts.append(item['æ¢æ–‡å…§å®¹'])
        if 'èªªæ˜' in item:
            parts.append(f"èªªæ˜ï¼š{item['èªªæ˜']}")
        if not parts:
            text = str(item)
            if len(text) > 500:
                text = text[:500] + "..."
            parts.append(text)
        return '\n'.join(parts)
    
    def build_law_knowledge_base(self):
        print("å»ºç«‹æ³•è¦çŸ¥è­˜åº«")
        law_documents = self.load_law_json()
        if os.path.exists(self.config.DOCUMENTS_DIR):
            print(f"æŸ¥è©¢å…¶ä»–æ³•è¦æ–‡ä»¶")
            loader = DirectoryLoader(
                self.config.DOCUMENTS_DIR,
                glob="**/*.txt",
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"}
            )
            text_documents = loader.load()
            print(f"ç¸½å…± {len(text_documents)} ä»½å…¶å®ƒæ–‡æœ¬æ–‡ä»¶")
            law_documents.extend(text_documents)
        if not law_documents:
            raise ValueError("æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ³•è¦")
        print(f"\nç¸½å…± {len(law_documents)} ä»½æ³•è¦æ–‡ä»¶")
        text_splitter = RecursiveCharacterTextSplitter(   # åˆ†å‰²æ–‡æœ¬
            chunk_size=self.config.CHUNK_SIZE,
            chunk_overlap=self.config.CHUNK_OVERLAP,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼Œ", " ", ""],
            length_function=len,
            is_separator_regex=False
        )
        texts = text_splitter.split_documents(law_documents)
        self.vector_db = Chroma.from_documents(          # 4.å‰µå»ºå‘é‡åº«
            documents=texts,
            embedding=self.embeddings,
            persist_directory=self.config.VECTOR_DB_DIR,
            collection_name="law_collection"
        )
        self._create_qa_chain()
        print(f"\nçŸ¥è­˜åº«å»ºç«‹å®Œæˆ")
    
    def _create_qa_chain(self):                         # å•ç­”æª¢ç´¢éˆ
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_db.as_retriever(
                search_type="similarity",
                search_kwargs={"k": self.config.TOP_K}
            ),
            return_source_documents=True,
            verbose=False
        )
    
    def load_existing_knowledge_base(self) -> bool:
        if os.path.exists(self.config.VECTOR_DB_DIR):
            print(f"\nå·²ç¶“æœ‰ç¾æœ‰çŸ¥è­˜åº«")
            try:
                self.vector_db = Chroma(
                    persist_directory=self.config.VECTOR_DB_DIR,
                    embedding_function=self.embeddings,
                    collection_name="law_collection"
                )
                self._create_qa_chain()
                print("Successful\n")
                return True
            except Exception as e:
                print(f"Failed: {e}\n")
                return False
        return False
    
    def review_contract(self, contract_path: str) -> Dict[str, Any]:
        print(f"å¯©æŸ¥å¥‘ç´„")
        try:  # è®€å–å¥‘ç´„å…§å®¹
            with open(contract_path, 'r', encoding='utf-8') as f:
                contract_content = f.read()
        except Exception as e:
            return {"Failed ": f"{e}"}
        
        review_questions = [          # æ§‹é€ å¯©æŸ¥å•é¡Œ(å¯æ–°å¢æ›´å¤šè§’åº¦æœ€å¤šäº”å€‹)
            f"""è«‹å¯©æŸ¥æ­¤å‹å·¥å¥‘ç´„ä¸¦æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š

                å¥‘ç´„å…§å®¹ï¼š
                {contract_content}

                è«‹æŒ‰æ­¤æ ¼å¼å›ç­”ï¼š

                ã€é•æ³•é …ç›®ã€‘
                - é …ç›®1ï¼šèªªæ˜
                - é …ç›®2ï¼šèªªæ˜

                ã€å»ºè­°ä¿®æ”¹ã€‘
                - å»ºè­°1
                - å»ºè­°2

                ã€åƒè€ƒæ³•æ¢ã€‘
                - æ³•æ¢1
                - æ³•æ¢2

                åªå›ç­”ä¸Šè¿°æ ¼å¼ï¼Œç°¡çŸ­æ‰¼è¦ã€‚
            """
        ]
        results = {
            "contract_path": contract_path,
            "contract_length": len(contract_content),
            "reviews": [],
            "related_laws": []
        }
        
        for idx, question in enumerate(review_questions, 1):        # åŸ·è¡Œå¤šè§’åº¦å¯©æŸ¥
            print(f"åŸ·è¡Œå¯©æŸ¥")
            try:
                start_time = time.time()
                result = self.qa_chain.invoke({"query": question})  # RAGï¼šæ‰¾æ³•æ¢  LLMï¼šå¯«æ³•å¾‹åˆ†æ
                end_time = time.time()
                answer = result.get("result", "")                   # LLM å›ç­”
                answer = self._clean_answer(answer)
                
                review_item = {
                    "question_type": f"å¯©æŸ¥è§’åº¦ {idx}",
                    "answer": answer,
                    "processing_time": end_time - start_time,
                    "source_laws": []
                }
                
                unique_law_set = set() 
                for doc in result.get("source_documents", []):
                    law_info = {
                        "content": doc.page_content[:300],
                        "source": doc.metadata.get("source", "Unknown"),
                        "metadata": doc.metadata
                    }
                    review_item["source_laws"].append(law_info)  # æ¯ä¸€å€‹å¯©æŸ¥è§’åº¦è‡ªå·±åƒè€ƒçš„æ³•è¦æ–‡ä»¶
                    unique_key = f"{law_info['source']}:{id(doc)}"
                    if unique_key not in unique_law_set:         # æ‰€æœ‰å¯©æŸ¥è§’åº¦ç”¨åˆ°çš„æ³•è¦ï¼Œä½†ä¸é‡è¤‡
                        unique_law_set.add(unique_key)
                        results["related_laws"].append(law_info)
                results["reviews"].append(review_item)
                print(f"Successful\n")
            except Exception as e:
                print(f"Failed: {e}\n")
                results["reviews"].append({
                    "question_type": f"å¯©æŸ¥è§’åº¦ {idx}",
                    "error": str(e)
                })
        return results
    
    def _clean_answer(self, answer: str) -> str:
        if "Use the following pieces of context" in answer:
            parts = answer.split("Helpful Answer:")
            if len(parts) > 1:
                answer = parts[-1]
        if "Question:" in answer:
            parts = answer.split("Question:")
            if len(parts) > 1:
                answer = parts[0]
        for marker in ["<|im_start|>", "<|im_end|>", "<s>", "</s>"]:
            answer = answer.replace(marker, "")
        return answer.strip()
    
    def batch_review_contracts(self, contracts_dir: str = None):  # æœ€å¾Œæœƒç”¢å‡ºä¸€å€‹ report.txt
        if contracts_dir is None:
            contracts_dir = self.config.CONTRACTS_DIR
        print(f"æƒæå¥‘ç´„ç›®éŒ„")
        contract_files = []

        for ext in ['*.txt', '*.doc', '*.docx']:
            contract_files.extend(Path(contracts_dir).glob(ext))
        if not contract_files:
            print(f"{contracts_dir} ä¸­æ²’æœ‰æ‰¾åˆ°å¥‘ç´„æ–‡ä»¶")
            return []
        print(f"æ‰¾åˆ° {len(contract_files)} ä»½å¥‘ç´„\n")

        all_results = []
        for idx, contract_file in enumerate(contract_files, 1):
            print(f"å¯©æŸ¥é€²åº¦: {idx}/{len(contract_files)}")            
            result = self.review_contract(str(contract_file))
            all_results.append(result)

        return all_results
    
    def generate_review_report(self, results: List[Dict], output_path: str = "report.txt"):
        """ç”Ÿæˆå¯©æŸ¥å ±å‘Š - å›ºå®šæ ¼å¼"""
        print(f"ç”Ÿæˆå¯©æŸ¥å ±å‘Š")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            # ========== æ¨™é¡Œ ==========
            f.write("å¤–ç±å‹å·¥è˜åƒ±å¥‘ç´„å¯©æŸ¥å ±å‘Š\n")
            f.write(f"{time.strftime('%Y/%m/%d')}\n\n")
            
            # ========== ç°¡ä»‹ ==========
            f.write("ç°¡ä»‹\n")
            f.write("æœ¬å ±å‘Šé‡å°æ‰€æä¾›ä¹‹å¤–ç±å‹å·¥è˜åƒ±å¥‘ç´„é€²è¡Œå…¨é¢æ€§æ³•è¦ç¬¦åˆåº¦å¯©æŸ¥ã€‚")
            f.write("ç¶“åˆ†æå¾Œï¼Œç™¼ç¾è©²å¥‘ç´„åœ¨å¤šé …æ¢æ¬¾ä¸Šèˆ‡ç¾è¡Œæ³•è¦æœ‰å‡ºå…¥ï¼Œä»¥ä¸‹ç‚ºè©³ç´°èªªæ˜ã€‚\n\n")
            
            # ========== è™•ç†æ¯ä»½å¥‘ç´„ ==========
            for contract_idx, result in enumerate(results, 1):
                if 'error' in result:
                    f.write(f"å¥‘ç´„ {contract_idx} åˆ†æå¤±æ•—: {result['error']}\n\n")
                    continue
                
                # ========== ç™¼ç¾äº‹é … ==========
                f.write("ç™¼ç¾äº‹é …\n")
                
                # å¾å¯©æŸ¥çµæœæå–å…§å®¹
                all_answers = []
                for review in result.get('reviews', []):
                    if 'answer' in review and not 'error' in review:
                        all_answers.append(review['answer'])
                
                # åˆä½µæ‰€æœ‰åˆ†æçµæœ
                combined_analysis = "\n\n".join(all_answers)
                
                # ğŸ¯ ä½¿ç”¨å›ºå®šçš„ç™¼ç¾äº‹é …æ ¼å¼
                findings = [
                    {
                        "title": "å·¥è³‡æ¢æ¬¾å¯©æŸ¥",
                        "content": self._extract_wage_info(combined_analysis)
                    },
                    {
                        "title": "å·¥æ™‚è¦å®šå¯©æŸ¥",
                        "content": self._extract_worktime_info(combined_analysis)
                    },
                    {
                        "title": "ä¼‘å‡è¦å®šå¯©æŸ¥",
                        "content": self._extract_leave_info(combined_analysis)
                    },
                    {
                        "title": "å…¶ä»–æ¢æ¬¾å¯©æŸ¥",
                        "content": self._extract_other_info(combined_analysis)
                    }
                ]
                
                for finding in findings:
                    if finding['content']:  # åªè¼¸å‡ºæœ‰å…§å®¹çš„é …ç›®
                        f.write(f"{finding['title']}\n")
                        f.write(f"{finding['content']}\n\n")
                
                # ========== å»ºè­° ==========
                f.write("å»ºè­°\n")
                
                recommendations = [
                    "å°‡åŸºæœ¬å·¥è³‡ä¿®æ­£ç‚ºç¬¦åˆæœ€æ–°æ³•å®šæ¨™æº–ï¼ˆæ¯æœˆä¸ä½æ–¼27,470å…ƒï¼‰",
                    "æ˜ç¢ºè¼‰æ˜åŠ ç­è²»è¨ˆç®—æ–¹å¼åŠæ”¯ä»˜æ™‚ç¨‹ï¼ˆå»¶é•·å·¥æ™‚å‰2å°æ™‚åŠ çµ¦1/3ï¼Œå†å»¶é•·åŠ çµ¦2/3ï¼‰",
                    "æª¢è¦–è†³å®¿è²»æ‰£é™¤æ˜¯å¦ç¬¦åˆæ³•è¦æ¯”ä¾‹ä¸Šé™",
                    "å»ºè­°å¢åˆ—å‹å·¥ç”³è¨´ç®¡é“åŠæ©Ÿåˆ¶",
                    "ç¢ºä¿å¥‘ç´„å…§å®¹ç¬¦åˆå°±æ¥­æœå‹™æ³•åŠå‹å‹•åŸºæº–æ³•ç›¸é—œè¦å®š"
                ]
                
                for idx, rec in enumerate(recommendations, 1):
                    f.write(f"{idx}.{rec}\n")
                
                f.write("\n")
                
                # ========== çµè«– ==========
                f.write("çµè«–\n")
                
                # ç°¡å–®è¨ˆç®—é•è¦é …ç›®ï¼ˆæ ¹æ“šé—œéµå­—ï¼‰
                violation_keywords = ['é•', 'ä¸ç¬¦', 'ä½æ–¼', 'æœª', 'ç¼º']
                violation_count = sum(1 for keyword in violation_keywords if keyword in combined_analysis)
                violation_count = min(violation_count, 3)  # æœ€å¤š3é …
                
                f.write(f"ç¶œä¸Šæ‰€è¿°ï¼Œè©²è˜åƒ±å¥‘ç´„å­˜åœ¨{violation_count}é …é‡å¤§é•è¦äº‹é …åŠ{len(recommendations)}é …å»ºè­°æ”¹å–„äº‹é …ã€‚")
                f.write("å»ºè­°é›‡ä¸»æ–¼ç°½è¨‚å¥‘ç´„å‰é€²è¡Œä¿®æ­£ï¼Œä»¥ç¢ºä¿ç¬¦åˆå‹å‹•æ³•è¦ä¸¦ä¿éšœå‹å·¥æ¬Šç›Šã€‚\n\n")
                
                # ========== åƒè€ƒæ³•è¦ï¼ˆå¯é¸ï¼‰ ==========
                if result.get('related_laws'):
                    f.write("\n" + "="*80 + "\n")
                    f.write("åƒè€ƒæ³•è¦æ¢æ–‡\n")
                    f.write("="*80 + "\n\n")
                    
                    for law_idx, law in enumerate(result.get('related_laws', [])[:5], 1):  # åªé¡¯ç¤ºå‰5æ¢
                        content = law['content']
                        if len(content) > 200:
                            content = content[:200] + "..."
                        f.write(f"{law_idx}.{content}\n")
                        f.write(f"   ä¾†æº: {law['source']}\n\n")
        
        print(f"å ±å‘Šå·²ä¿å­˜è‡³: {output_path}\n")


    def _extract_wage_info(self, text: str) -> str:
        """æå–å·¥è³‡ç›¸é—œè³‡è¨Š"""
        if 'å·¥è³‡' in text or 'è–ªè³‡' in text or '27470' in text or '27,470' in text:
            # å˜—è©¦æ‰¾å‡ºå·¥è³‡ç›¸é—œæ®µè½
            lines = text.split('\n')
            for line in lines:
                if 'å·¥è³‡' in line or 'è–ªè³‡' in line:
                    return line.strip()
            return "å¥‘ç´„ä¸­æœ‰æåŠå·¥è³‡æ¢æ¬¾ï¼Œè«‹ç¢ºèªæ˜¯å¦ç¬¦åˆæœ€ä½å·¥è³‡æ¨™æº–ï¼ˆæ¯æœˆ27,470å…ƒï¼‰ã€‚"
        return "æœªæ˜ç¢ºç™¼ç¾å·¥è³‡ç›¸é—œå•é¡Œï¼Œå»ºè­°ä»éœ€ç¢ºèªæ˜¯å¦ç¬¦åˆåŸºæœ¬å·¥è³‡æ¨™æº–ã€‚"


    def _extract_worktime_info(self, text: str) -> str:
        """æå–å·¥æ™‚ç›¸é—œè³‡è¨Š"""
        if 'å·¥æ™‚' in text or 'å·¥ä½œæ™‚é–“' in text or 'åŠ ç­' in text:
            lines = text.split('\n')
            for line in lines:
                if 'å·¥æ™‚' in line or 'å·¥ä½œæ™‚é–“' in line or 'åŠ ç­' in line:
                    return line.strip()
            return "å¥‘ç´„ä¸­æœ‰æåŠå·¥æ™‚è¦å®šï¼Œè«‹ç¢ºèªæ˜¯å¦ç¬¦åˆæ¯æ—¥8å°æ™‚ã€æ¯é€±40å°æ™‚çš„æ¨™æº–ã€‚"
        return "æœªæ˜ç¢ºç™¼ç¾å·¥æ™‚ç›¸é—œå•é¡Œï¼Œå»ºè­°ç¢ºèªå·¥æ™‚åŠåŠ ç­è²»è¨ˆç®—æ–¹å¼æ˜¯å¦æ˜ç¢ºã€‚"


    def _extract_leave_info(self, text: str) -> str:
        """æå–ä¼‘å‡ç›¸é—œè³‡è¨Š"""
        if 'ä¼‘å‡' in text or 'ä¾‹å‡' in text or 'ä¼‘æ¯' in text:
            lines = text.split('\n')
            for line in lines:
                if 'ä¼‘å‡' in line or 'ä¾‹å‡' in line:
                    return line.strip()
            return "å¥‘ç´„ä¸­æœ‰æåŠä¼‘å‡è¦å®šï¼Œè«‹ç¢ºèªæ˜¯å¦ç¬¦åˆæ¯ä¸ƒæ—¥æ‡‰æœ‰å…©æ—¥ä¼‘æ¯çš„è¦å®šã€‚"
        return "æœªæ˜ç¢ºç™¼ç¾ä¼‘å‡ç›¸é—œå•é¡Œï¼Œå»ºè­°ç¢ºèªä¼‘å‡åˆ¶åº¦æ˜¯å¦å®Œæ•´ã€‚"


    def _extract_other_info(self, text: str) -> str:
        """æå–å…¶ä»–é‡è¦è³‡è¨Š"""
        keywords = ['é•ç´„é‡‘', 'è†³å®¿', 'ä¿éšª', 'å‹å¥ä¿']
        for keyword in keywords:
            if keyword in text:
                lines = text.split('\n')
                for line in lines:
                    if keyword in line:
                        return line.strip()
        return "å…¶ä»–æ¢æ¬¾è«‹ä¾å°±æ¥­æœå‹™æ³•åŠå‹å‹•åŸºæº–æ³•ç›¸é—œè¦å®šé€²è¡Œæª¢è¦–ã€‚"

def main():
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    print("å¤–ç±å‹å·¥å¥‘ç´„å¯©æŸ¥ç³»çµ±")
    config = Config()
    try:
        system = LaborContractReviewSystem(config)
        if not system.load_existing_knowledge_base():  # å»ºç«‹æ³•è¦çŸ¥è­˜åº«
            system.build_law_knowledge_base()
        print("é–‹å§‹å¯©æŸ¥å¥‘ç´„")

        if len(sys.argv) > 1:  # å–®ä¸€æª”æ¡ˆæ¨¡å¼
            contract_file = sys.argv[1]
            print(f"è™•ç†å–®ä¸€å¥‘ç´„: {contract_file}")
            if not os.path.exists(contract_file):
                print(f"Failed: {contract_file}")
                sys.exit(1)
            result = system.review_contract(contract_file)
            system.generate_review_report([result], "report.txt")
            print("Successfulï¼")
            
        else:  # æ‰¹æ¬¡è™•ç†æ¨¡å¼
            results = system.batch_review_contracts()
            if results:
                system.generate_review_report(results, "report.txt")
                print("Successfulï¼")
                print(f"å…±å¯©æŸ¥ {len(results)} ä»½å¥‘ç´„")
            else:
                print("Failed")
                sys.exit(1)
    except Exception as e:
        print(f"\nFailed: {e}")
        import traceback
        traceback.print_exc()
if __name__ == "__main__":
    main()